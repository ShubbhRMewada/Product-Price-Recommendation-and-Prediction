{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82461fb",
   "metadata": {},
   "source": [
    "# Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589317b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ-7qovRrodq",
    "outputId": "ef79fceb-0fb0-4e1e-944d-3c794e23e338"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading vader_lexicon: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\nnd\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\nnd\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nnd\\anaconda3\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nnd\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\nnd\\anaconda3\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: Levenshtein==0.21.1 in c:\\users\\nnd\\anaconda3\\lib\\site-packages (from python-Levenshtein) (0.21.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in c:\\users\\nnd\\anaconda3\\lib\\site-packages (from Levenshtein==0.21.1->python-Levenshtein) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import collections\n",
    "from wordcloud import STOPWORDS\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import string\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "\n",
    "!pip install gensim\n",
    "!pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd790dd",
   "metadata": {
    "id": "odaOEnmbstYA"
   },
   "source": [
    "# Data Acquisition\n",
    "\n",
    "## Follow these steps to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d22572",
   "metadata": {
    "id": "WXoxVH1Kr13s"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_table(\"train.tsv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e855a",
   "metadata": {
    "id": "ZyqGg7iPswL9"
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a37ba14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQCw8Zc1GEZR",
    "outputId": "8167c86a-c7a6-4798-9be8-5751c75bc68f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing redundant prices:  (1482535, 8)\n",
      "After removing redundant prices:  (1481661, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before removing redundant prices: \",train_data.shape)\n",
    "train_data = train_data[train_data['price'] > 0].reset_index(drop=True)\n",
    "print(\"After removing redundant prices: \",train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb9c5b",
   "metadata": {
    "id": "Ionnv1Dcsy_E"
   },
   "source": [
    "# Basic Preprocessing Steps\n",
    "   ### Lower Casing\n",
    "   ### Remove HTML tags\n",
    "   ### Remove Urls\n",
    "   ### Remove Punctuation\n",
    "   ### Chat Word Treatment\n",
    "   ### Spelling Correction\n",
    "   ### Removing Stop Words\n",
    "   ### Handling Emojis\n",
    "   ### Tokenization\n",
    "   ### Stemming\n",
    "   ### Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212c84b7",
   "metadata": {
    "id": "q11wHXR2syfk"
   },
   "outputs": [],
   "source": [
    "#Lower Casing\n",
    "\n",
    "#Remove HTML tags\n",
    "def striphtml(data):\n",
    "    data = str(data)\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', data)\n",
    "\n",
    "\n",
    "#Remove URLs\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "#Sub-Categories\n",
    "def handle_category(data):    \n",
    "    cat1=[]\n",
    "    cat2=[]\n",
    "    cat3=[]\n",
    "    i=0\n",
    "    for row in data:\n",
    "        try:\n",
    "            categories=row.split('/')\n",
    "        except:\n",
    "            categories=['','','']\n",
    "        cat1.append(categories[0])\n",
    "        cat2.append(categories[1])\n",
    "        cat3.append(categories[2])\n",
    "        i+=1\n",
    "    return cat1,cat2,cat3\n",
    "\n",
    "\n",
    "\n",
    "train_data['name'] = train_data['name'].str.lower()\n",
    "train_data['category_name'] = train_data['category_name'].str.lower()\n",
    "train_data['item_description'] = train_data['item_description'].str.lower()\n",
    "train_data['item_description'] = train_data['item_description'].apply(striphtml)\n",
    "train_data['item_description'] = train_data['item_description'].apply(remove_url)    \n",
    "\n",
    "\n",
    "c1,c2,c3=handle_category(train_data['category_name'])\n",
    "train_data['sub_category1']=c1\n",
    "train_data['sub_category2']=c2\n",
    "train_data['sub_category3']=c3\n",
    "train_data['item_description'].fillna(value='No description given',inplace=True)\n",
    "train_data['brand_name'].fillna(value='Not known',inplace=True)\n",
    "train_data = train_data.drop(['category_name'], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6d9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the string-based value column as one.\n",
    "\n",
    "selected_columns = ['name', 'brand_name', 'item_description', 'sub_category1', 'sub_category2', 'sub_category3']\n",
    "train_data['concatenated_description'] = train_data[selected_columns].apply(lambda x:' '.join(x), axis=1)\n",
    "train_data = train_data.drop(selected_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceaea90f",
   "metadata": {
    "id": "IIjxwWpqvV3Z"
   },
   "outputs": [],
   "source": [
    "#Remove Punctuations\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','', exclude))\n",
    "\n",
    "train_data['concatenated_description'] = train_data['concatenated_description'].apply(remove_punc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19a1003",
   "metadata": {
    "id": "67iXJU9yw3_U"
   },
   "outputs": [],
   "source": [
    "# Chat word treatment -- ROFL, LMAO, WTH, GN, ASAP, etc\n",
    "\n",
    "fileName = \"slang.txt\"\n",
    "accessMode = \"r\"\n",
    "with open(fileName, accessMode) as data:\n",
    "    values = data.readlines()\n",
    "\n",
    "dictionary = dict()\n",
    "\n",
    "for i in values:\n",
    "    key,value = i.split(\"=\")\n",
    "    dictionary[key] = value\n",
    "\n",
    "def translator(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in dictionary:\n",
    "            new_text.append(dictionary[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "train_data['concatenated_description'] = train_data['concatenated_description'].apply(translator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbff8f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t601tXCD6eFC",
    "outputId": "65f0113b-44b9-4cca-f2aa-452a76788581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "# Handling Stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # converts the words in word_tokens to lower case and then checks whether\n",
    "    #they are present in stop_words or not\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "\n",
    "train_data['concatenated_description'] = train_data['concatenated_description'].apply(remove_stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8d2304",
   "metadata": {
    "id": "RhRhyA59KSv1"
   },
   "outputs": [],
   "source": [
    "# Text Normalization\n",
    "# Stemming\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenization(text):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    l = tokenizer.tokenize(text)\n",
    "    return \" \".join(l)\n",
    "\n",
    "train_data['concatenated_description'] = train_data['concatenated_description'].apply(tokenization)\n",
    "train_data.to_csv('preprocessed_tokenized_training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39bc60",
   "metadata": {
    "id": "U6fLkerFr2zU"
   },
   "source": [
    "# Feature Extraction from text\n",
    "# Beware of the saying \"Garbage in, Garbage Out\"\n",
    "#  -- The efficiency of your model depends on the features that you have extracted from the data. If the features are garbage, you're model despite being extremely good, will give you garbage output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
