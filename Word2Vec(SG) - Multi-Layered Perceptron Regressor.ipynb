{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Imports and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading vader_lexicon: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: Levenshtein==0.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-Levenshtein) (0.21.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Levenshtein==0.21.1->python-Levenshtein) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import collections\n",
    "from wordcloud import STOPWORDS\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import string\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "!pip install gensim\n",
    "!pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Here, Initiates the Training Phase of Model.\n",
    "\n",
    "<strong><p style=\"color:blue;font-size:20px;\"> Data Preprocessing is completed in the file named \"Data Preprocessing.ipynb\".</p>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "pQVgxtersseC"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"preprocessed_tokenized_training_data.csv\")\n",
    "train_data['concatenated_description'] = train_data['concatenated_description'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GInA3QoeJl9c"
   },
   "outputs": [],
   "source": [
    "train_data['concatenated_description'] = train_data['concatenated_description'].apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>train_id</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>concatenated_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[mlb, cincinnati, reds, shirt, size, xl, not, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[razer, blackwidow, chroma, keyboard, razer, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[avaviv, blouse, target, adorable, top, hint, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[leather, horse, statues, not, known, new, tag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[gold, plated, rose, not, known, complete, cer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481656</th>\n",
       "      <td>1481656</td>\n",
       "      <td>1482530</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[free, people, inspired, dress, free, people, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481657</th>\n",
       "      <td>1481657</td>\n",
       "      <td>1482531</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[little, mermaid, handmade, dress, disney, lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481658</th>\n",
       "      <td>1481658</td>\n",
       "      <td>1482532</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[day, fix, containers, eating, plan, not, know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481659</th>\n",
       "      <td>1481659</td>\n",
       "      <td>1482533</td>\n",
       "      <td>3</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[world, markets, lanterns, not, known, one, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481660</th>\n",
       "      <td>1481660</td>\n",
       "      <td>1482534</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[brand, new, lux, de, ville, wallet, not, know...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1481661 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  train_id  item_condition_id  price  shipping  \\\n",
       "0                 0         0                  3   10.0         1   \n",
       "1                 1         1                  3   52.0         0   \n",
       "2                 2         2                  1   10.0         1   \n",
       "3                 3         3                  1   35.0         1   \n",
       "4                 4         4                  1   44.0         0   \n",
       "...             ...       ...                ...    ...       ...   \n",
       "1481656     1481656   1482530                  2   20.0         1   \n",
       "1481657     1481657   1482531                  2   14.0         0   \n",
       "1481658     1481658   1482532                  2   12.0         0   \n",
       "1481659     1481659   1482533                  3   45.0         1   \n",
       "1481660     1481660   1482534                  1   22.0         0   \n",
       "\n",
       "                                  concatenated_description  \n",
       "0        [mlb, cincinnati, reds, shirt, size, xl, not, ...  \n",
       "1        [razer, blackwidow, chroma, keyboard, razer, k...  \n",
       "2        [avaviv, blouse, target, adorable, top, hint, ...  \n",
       "3        [leather, horse, statues, not, known, new, tag...  \n",
       "4        [gold, plated, rose, not, known, complete, cer...  \n",
       "...                                                    ...  \n",
       "1481656  [free, people, inspired, dress, free, people, ...  \n",
       "1481657  [little, mermaid, handmade, dress, disney, lit...  \n",
       "1481658  [day, fix, containers, eating, plan, not, know...  \n",
       "1481659  [world, markets, lanterns, not, known, one, se...  \n",
       "1481660  [brand, new, lux, de, ville, wallet, not, know...  \n",
       "\n",
       "[1481661 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<strong><p style=\"color:red;font-size:30px;\">Run the next cell only if you want to train the model.</p><br><br>\n",
    "<strong><p style=\"color:green;font-size:30px;\">If already trained, Skip to the next cell and directly load the model.</p><br><br>\n",
    "<strong><p style=\"color:blue;font-size:30px;\">Note: I have trained the model and saved it as \"word2vec_sg_model\"</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QpwHNf5dJ1Sv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab for Model-1: 41.98 mins\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Build the Word2Vec model\n",
    "# Skip Gram\n",
    "# Train the Word2Vec model\n",
    "\n",
    "model1 = Word2Vec(vector_size=200, window=5, min_count=1, sg=1, workers=cores-1)\n",
    "model1.build_vocab(train_data['concatenated_description'], progress_per=1000)\n",
    "t = time.time()\n",
    "model1.train(train_data['concatenated_description'], total_examples=model1.corpus_count, epochs=20)\n",
    "print('Time to build vocab for Model-1: {} mins'.format(round((time.time() - t) / 60, 2)))\n",
    "model1.save(\"word2vec_sg_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><p style=\"color:blue;font-size:30px;\">Load the model.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYAkgFhiOytT",
    "outputId": "d42bb6bc-14be-4a56-c6e5-8ca71172fd27"
   },
   "outputs": [],
   "source": [
    "#Loading a pretrained model\n",
    "\n",
    "model1 = Word2Vec.load(\"word2vec_sg_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec1(doc):\n",
    "    return np.mean([model1.wv[word] for word in doc if word in model1.wv.index_to_key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_model1 = train_data.sample(frac=0.03, random_state=1)\n",
    "\n",
    "series = train_data_model1.concatenated_description.apply(avg_word2vec1)\n",
    "\n",
    "df = pd.DataFrame(series.apply(pd.Series))\n",
    "\n",
    "train_data_model1 = pd.concat([train_data_model1, df], axis=1)\n",
    "\n",
    "train_data_model1 = train_data_model1.drop(['concatenated_description'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><p style=\"color:blue;font-size:30px;\">Saving the data set for future use.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model1.to_csv('avgword2vec_sg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model1 = pd.read_csv('avgword2vec_sg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<strong><p style=\"color:green;font-size:30px;\">\"avgword2vec_sg.csv\" Will be used for \"train_test_split\"</p>\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We've our dataset ready for the Model-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d22572",
   "metadata": {
    "id": "WXoxVH1Kr13s"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('training_data_for_Model1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f19faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b2ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['price']\n",
    "X = data.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fcdb9b",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e69f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=5, max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=5, max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=5, max_iter=2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=5,max_iter=2000)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1014.8595587715881\n",
      "Root Mean Squared Error (RMSE): 31.85686046633579\n",
      "Mean Absolute Error (MAE): 16.26224271749436\n",
      "R-squared (RÂ²): 0.2654078346837121\n",
      "Explained Variance Score: 0.26621013883628675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# R-squared (RÂ²) or Coefficient of Determination\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (RÂ²):\", r2)\n",
    "\n",
    "# Explained Variance Score\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "print(\"Explained Variance Score:\", explained_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE: 1228.9066361848895\n",
      "Root Mean Squared Error (RMSE): 35.055764664101815\n",
      "Standard Deviation of MSE: 301.8013828188631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create an MLPRegressor model with desired hyperparameters\n",
    "model = mlp\n",
    "\n",
    "# Define the input features X and the target variable y\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "num_folds = 5\n",
    "scores = cross_val_score(model, X, y, cv=num_folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Cross-validation returns negative mean squared error, so we take the absolute values\n",
    "mse_scores = -scores\n",
    "\n",
    "# Compute the mean and standard deviation of the cross-validation scores\n",
    "mean_mse = mse_scores.mean()\n",
    "std_mse = mse_scores.std()\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_mse)\n",
    "# Print the results\n",
    "print(\"Mean MSE:\", mean_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Standard Deviation of MSE:\", std_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'sg_model1.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(mlp, file)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
