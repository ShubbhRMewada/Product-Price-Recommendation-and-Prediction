{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ-7qovRrodq",
    "outputId": "ef79fceb-0fb0-4e1e-944d-3c794e23e338",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install gensim\n",
    "# !pip install python-Levenshtein\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import collections\n",
    "# from wordcloud import STOPWORDS\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import string\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the next cell only if you want to train the model. \n",
    "# If already trained, Skip to the next cell and directly load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pQVgxtersseC"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"preprocessed_tokenized_training_data.csv\")\n",
    "train_data['concatenated_description'] = train_data['concatenated_description'].map(str)\n",
    "train_data_idf = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GInA3QoeJl9c"
   },
   "outputs": [],
   "source": [
    "train_data['concatenated_description'] = train_data['concatenated_description'].apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QpwHNf5dJ1Sv"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Build the Word2Vec model\n",
    "# Continous bag of words\n",
    "# Train the Word2Vec model\n",
    "\n",
    "model2 = Word2Vec(vector_size=200, window=5, min_count=1, sg=0, workers=cores-1)\n",
    "model2.build_vocab(train_data['concatenated_description'], progress_per=1000)\n",
    "t = time.time()\n",
    "model2.train(train_data['concatenated_description'], total_examples=model2.corpus_count, epochs=20)\n",
    "print('Time to build vocab for Model-2: {} mins'.format(round((time.time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jIy1sBOEOyu7"
   },
   "outputs": [],
   "source": [
    "model2.save(\"word2vec_cbow_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One can directly load the model from here instead of training the models again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYAkgFhiOytT",
    "outputId": "d42bb6bc-14be-4a56-c6e5-8ca71172fd27"
   },
   "outputs": [],
   "source": [
    "#Loading a pretrained model\n",
    "\n",
    "model2 = Word2Vec.load(\"word2vec_cbow_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec2(doc):\n",
    "    return np.mean([model2.wv[word] for word in doc if word in model2.wv.index_to_key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2 = train_data.copy()\n",
    "\n",
    "series = train_data_model2.concatenated_description.apply(avg_word2vec2)\n",
    "\n",
    "df = pd.DataFrame(series.apply(pd.Series))\n",
    "\n",
    "train_data_model2 = pd.concat([train_data_model2, df], axis=1)\n",
    "\n",
    "train_data_model2 = train_data_model2.drop(['concatenated_description'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2.to_csv('avgword2vec_cbow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2 = pd.read_csv('avgword2vec_sg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'train_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037552</td>\n",
       "      <td>0.023915</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.024870</td>\n",
       "      <td>0.125262</td>\n",
       "      <td>-0.038266</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261896</td>\n",
       "      <td>0.143941</td>\n",
       "      <td>-0.128030</td>\n",
       "      <td>0.080459</td>\n",
       "      <td>0.144991</td>\n",
       "      <td>0.017747</td>\n",
       "      <td>0.205571</td>\n",
       "      <td>-0.201369</td>\n",
       "      <td>-0.035721</td>\n",
       "      <td>-0.056952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.232859</td>\n",
       "      <td>-0.038133</td>\n",
       "      <td>0.074237</td>\n",
       "      <td>0.100791</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>-0.013040</td>\n",
       "      <td>0.028851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251430</td>\n",
       "      <td>0.066081</td>\n",
       "      <td>0.096789</td>\n",
       "      <td>0.106077</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.047183</td>\n",
       "      <td>0.072342</td>\n",
       "      <td>-0.355895</td>\n",
       "      <td>-0.056492</td>\n",
       "      <td>-0.148951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033843</td>\n",
       "      <td>-0.001777</td>\n",
       "      <td>-0.164303</td>\n",
       "      <td>0.292715</td>\n",
       "      <td>0.050827</td>\n",
       "      <td>0.184387</td>\n",
       "      <td>0.174933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230903</td>\n",
       "      <td>0.064281</td>\n",
       "      <td>0.136571</td>\n",
       "      <td>-0.133916</td>\n",
       "      <td>0.242645</td>\n",
       "      <td>-0.154565</td>\n",
       "      <td>0.043090</td>\n",
       "      <td>-0.182490</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>-0.233542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.277575</td>\n",
       "      <td>-0.176075</td>\n",
       "      <td>0.103876</td>\n",
       "      <td>0.312039</td>\n",
       "      <td>-0.002630</td>\n",
       "      <td>-0.312623</td>\n",
       "      <td>0.096224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320663</td>\n",
       "      <td>0.184249</td>\n",
       "      <td>0.067134</td>\n",
       "      <td>0.063168</td>\n",
       "      <td>0.386055</td>\n",
       "      <td>0.023002</td>\n",
       "      <td>0.048363</td>\n",
       "      <td>-0.309135</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>-0.173594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042444</td>\n",
       "      <td>0.112054</td>\n",
       "      <td>0.135623</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.153029</td>\n",
       "      <td>-0.083521</td>\n",
       "      <td>0.179250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271193</td>\n",
       "      <td>0.262207</td>\n",
       "      <td>-0.010959</td>\n",
       "      <td>0.115632</td>\n",
       "      <td>-0.048844</td>\n",
       "      <td>-0.130186</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>-0.207454</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>-0.216210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22220</th>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.074463</td>\n",
       "      <td>0.122469</td>\n",
       "      <td>0.416734</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>0.094992</td>\n",
       "      <td>-0.172024</td>\n",
       "      <td>0.144668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089671</td>\n",
       "      <td>-0.003719</td>\n",
       "      <td>-0.180867</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.097486</td>\n",
       "      <td>0.138730</td>\n",
       "      <td>0.137916</td>\n",
       "      <td>-0.007709</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>-0.166371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22221</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086083</td>\n",
       "      <td>-0.024563</td>\n",
       "      <td>-0.018871</td>\n",
       "      <td>0.173806</td>\n",
       "      <td>0.201046</td>\n",
       "      <td>-0.288738</td>\n",
       "      <td>-0.076850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335237</td>\n",
       "      <td>-0.205561</td>\n",
       "      <td>0.072868</td>\n",
       "      <td>-0.055953</td>\n",
       "      <td>0.042459</td>\n",
       "      <td>0.036656</td>\n",
       "      <td>0.058271</td>\n",
       "      <td>-0.196131</td>\n",
       "      <td>0.020433</td>\n",
       "      <td>-0.209770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22222</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146066</td>\n",
       "      <td>-0.121376</td>\n",
       "      <td>0.111775</td>\n",
       "      <td>0.098236</td>\n",
       "      <td>0.063539</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>-0.072577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291403</td>\n",
       "      <td>0.053669</td>\n",
       "      <td>0.114085</td>\n",
       "      <td>-0.105494</td>\n",
       "      <td>0.046369</td>\n",
       "      <td>0.136659</td>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.212911</td>\n",
       "      <td>0.126016</td>\n",
       "      <td>0.030037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22223</th>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.041727</td>\n",
       "      <td>-0.142778</td>\n",
       "      <td>0.097017</td>\n",
       "      <td>0.209473</td>\n",
       "      <td>-0.015237</td>\n",
       "      <td>-0.066154</td>\n",
       "      <td>-0.294085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369271</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.175795</td>\n",
       "      <td>-0.047760</td>\n",
       "      <td>0.027296</td>\n",
       "      <td>0.038357</td>\n",
       "      <td>0.118579</td>\n",
       "      <td>-0.114327</td>\n",
       "      <td>-0.092380</td>\n",
       "      <td>0.047048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22224</th>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.147853</td>\n",
       "      <td>0.040602</td>\n",
       "      <td>-0.105794</td>\n",
       "      <td>0.259467</td>\n",
       "      <td>0.137257</td>\n",
       "      <td>-0.029217</td>\n",
       "      <td>0.169930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259493</td>\n",
       "      <td>0.163386</td>\n",
       "      <td>0.048273</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.150644</td>\n",
       "      <td>0.021865</td>\n",
       "      <td>0.067533</td>\n",
       "      <td>-0.199626</td>\n",
       "      <td>0.100950</td>\n",
       "      <td>-0.239893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22225 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id  price  shipping         0         1         2  \\\n",
       "0                      4    6.0         1  0.037552  0.023915  0.005228   \n",
       "1                      1   38.0         0  0.232859 -0.038133  0.074237   \n",
       "2                      3   19.0         1 -0.033843 -0.001777 -0.164303   \n",
       "3                      3   13.0         1  0.277575 -0.176075  0.103876   \n",
       "4                      2   14.0         1  0.042444  0.112054  0.135623   \n",
       "...                  ...    ...       ...       ...       ...       ...   \n",
       "22220                  3   21.0         0 -0.074463  0.122469  0.416734   \n",
       "22221                  1   10.0         0  0.086083 -0.024563 -0.018871   \n",
       "22222                  1   10.0         0  0.146066 -0.121376  0.111775   \n",
       "22223                  1   12.0         0 -0.041727 -0.142778  0.097017   \n",
       "22224                  1   12.0         0  0.147853  0.040602 -0.105794   \n",
       "\n",
       "              3         4         5         6  ...       190       191  \\\n",
       "0      0.024870  0.125262 -0.038266  0.113200  ...  0.261896  0.143941   \n",
       "1      0.100791  0.023038 -0.013040  0.028851  ...  0.251430  0.066081   \n",
       "2      0.292715  0.050827  0.184387  0.174933  ...  0.230903  0.064281   \n",
       "3      0.312039 -0.002630 -0.312623  0.096224  ...  0.320663  0.184249   \n",
       "4      0.246667  0.153029 -0.083521  0.179250  ...  0.271193  0.262207   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "22220  0.020856  0.094992 -0.172024  0.144668  ...  0.089671 -0.003719   \n",
       "22221  0.173806  0.201046 -0.288738 -0.076850  ...  0.335237 -0.205561   \n",
       "22222  0.098236  0.063539  0.013970 -0.072577  ...  0.291403  0.053669   \n",
       "22223  0.209473 -0.015237 -0.066154 -0.294085  ...  0.369271  0.002607   \n",
       "22224  0.259467  0.137257 -0.029217  0.169930  ...  0.259493  0.163386   \n",
       "\n",
       "            192       193       194       195       196       197       198  \\\n",
       "0     -0.128030  0.080459  0.144991  0.017747  0.205571 -0.201369 -0.035721   \n",
       "1      0.096789  0.106077 -0.000116  0.047183  0.072342 -0.355895 -0.056492   \n",
       "2      0.136571 -0.133916  0.242645 -0.154565  0.043090 -0.182490  0.015514   \n",
       "3      0.067134  0.063168  0.386055  0.023002  0.048363 -0.309135  0.004195   \n",
       "4     -0.010959  0.115632 -0.048844 -0.130186  0.229600 -0.207454  0.013241   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "22220 -0.180867 -0.000074 -0.097486  0.138730  0.137916 -0.007709  0.017316   \n",
       "22221  0.072868 -0.055953  0.042459  0.036656  0.058271 -0.196131  0.020433   \n",
       "22222  0.114085 -0.105494  0.046369  0.136659  0.182144 -0.212911  0.126016   \n",
       "22223  0.175795 -0.047760  0.027296  0.038357  0.118579 -0.114327 -0.092380   \n",
       "22224  0.048273  0.004627  0.150644  0.021865  0.067533 -0.199626  0.100950   \n",
       "\n",
       "            199  \n",
       "0     -0.056952  \n",
       "1     -0.148951  \n",
       "2     -0.233542  \n",
       "3     -0.173594  \n",
       "4     -0.216210  \n",
       "...         ...  \n",
       "22220 -0.166371  \n",
       "22221 -0.209770  \n",
       "22222  0.030037  \n",
       "22223  0.047048  \n",
       "22224 -0.239893  \n",
       "\n",
       "[22225 rows x 203 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"avgword2vec_sg.csv\" --> Will be used for \"train_test_split\", Contains 0.03 of entire dataset. \n",
    "## Generated from extracting dataset \"preprocessed_tokenized_training_data.csv\" and then applying \"word2vec_cbow_model\" on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We've our dataset ready for the Model-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data_model2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6e69f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['price']\n",
    "X = data.drop(columns=['price'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "X_test = X_test[:3500]\n",
    "y_test = y_test[:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=175)\n",
    "X_K_train = kpca.fit_transform(X_train)\n",
    "X_K_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.182 total time=  42.2s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.287 total time=  42.0s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.262 total time=  42.1s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.163 total time=  42.4s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.211 total time=  42.1s\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=-0.136 total time=  30.0s\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.222 total time=  31.1s\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.182 total time=  30.1s\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.062 total time=  27.8s\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.072 total time=  27.4s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.110 total time=  28.1s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.302 total time=  28.5s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.269 total time=  29.0s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.144 total time=  31.0s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.255 total time=  28.4s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.120 total time=  18.9s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.282 total time=  18.4s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.269 total time=  18.4s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.117 total time=  18.4s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.173 total time=  18.6s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.002 total time=  43.4s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.166 total time=  43.2s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.145 total time=  44.3s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.041 total time=  43.0s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.076 total time=  44.6s\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.106 total time=  14.6s\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.177 total time=  14.7s\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.174 total time=  14.6s\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.047 total time=  14.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.119 total time=  14.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.036 total time= 1.1min\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.287 total time= 1.1min\n",
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.283 total time= 1.1min\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.125 total time= 1.1min\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.195 total time= 1.1min\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.123 total time=  34.4s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.290 total time=  35.8s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.257 total time=  50.2s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.156 total time=  35.4s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.212 total time=  34.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=-0.088 total time=  34.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.268 total time=  34.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.156 total time=  34.2s\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.132 total time=  35.6s\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.077 total time=  34.7s\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.088 total time=  10.6s\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.232 total time=  10.5s\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.239 total time=  10.6s\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.120 total time=  10.5s\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.189 total time=  10.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.025 total time=  22.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.286 total time=  22.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.288 total time=  22.4s\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.125 total time=  22.5s\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.213 total time=  22.4s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.178 total time=  17.7s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.228 total time=  17.7s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.225 total time=  18.2s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.114 total time=  17.7s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.155 total time=  18.7s\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.108 total time=  49.1s\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.284 total time=  49.0s\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.283 total time=  48.6s\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.142 total time=  49.0s\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.208 total time=  49.0s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=-0.060 total time=  28.4s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.164 total time=  28.2s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.148 total time=  27.0s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.043 total time=  27.4s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.115 total time=  27.3s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=-0.013 total time=  10.6s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.225 total time=  10.5s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.255 total time=  10.5s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.066 total time=  10.6s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.167 total time=  10.6s\n",
      "Best Hyperparameters: {'subsample': 0.9, 'reg_lambda': 0.001, 'reg_alpha': 0.1, 'n_estimators': 400, 'min_child_weight': 7, 'max_depth': 7, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 0.6}\n",
      "Mean Squared Error: 1434.4592011139548\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate for boosting\n",
    "    'max_depth': range(3, 10),  # Maximum tree depth\n",
    "    'n_estimators': [100, 200, 300, 400],  # Number of boosted trees\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.1, 1],  # L1 regularization term on weights\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.1, 1],  # L2 regularization term on weights\n",
    "    'min_child_weight': [1, 3, 5, 7],  # Minimum sum of instance weight needed in a child\n",
    "}\n",
    "\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist, n_iter=15, cv=5, random_state=42, verbose=3)\n",
    "\n",
    "random_search.fit(X_K_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "y_pred = best_model.predict(X_K_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1434.4592011139548\n",
      "Root Mean Squared Error (RMSE): 37.87425512289258\n",
      "Mean Absolute Error (MAE): 15.126183448655265\n",
      "R-squared (R²): 0.1493341390803118\n",
      "Explained Variance Score: 0.15023684282710437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# R-squared (R²) or Coefficient of Determination\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R²):\", r2)\n",
    "\n",
    "# Explained Variance Score\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "print(\"Explained Variance Score:\", explained_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE: 1445.5082288935332\n",
      "Root Mean Squared Error (RMSE): 38.01983993776845\n",
      "Standard Deviation of MSE: 367.0868743231442\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_folds = 5\n",
    "scores = cross_val_score(xgb, X_K_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Cross-validation returns negative mean squared error, so we take the absolute values\n",
    "mse_scores = -scores\n",
    "\n",
    "# Compute the mean and standard deviation of the cross-validation scores\n",
    "mean_mse = mse_scores.mean()\n",
    "std_mse = mse_scores.std()\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_mse)\n",
    "# Print the results\n",
    "print(\"Mean MSE:\", mean_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Standard Deviation of MSE:\", std_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'sg_model6.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
