{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ-7qovRrodq",
    "outputId": "ef79fceb-0fb0-4e1e-944d-3c794e23e338",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install gensim\n",
    "# !pip install python-Levenshtein\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import collections\n",
    "# from wordcloud import STOPWORDS\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# nltk.download('vader_lexicon')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import string\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import multiprocessing\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the next cell only if you want to train the model. \n",
    "# If already trained, Skip to the next cell and directly load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pQVgxtersseC"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"preprocessed_tokenized_training_data.csv\")\n",
    "train_data['concatenated_description'] = train_data['concatenated_description'].map(str)\n",
    "train_data_idf = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GInA3QoeJl9c"
   },
   "outputs": [],
   "source": [
    "train_data['concatenated_description'] = train_data['concatenated_description'].apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QpwHNf5dJ1Sv"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Build the Word2Vec model\n",
    "# Continous bag of words\n",
    "# Train the Word2Vec model\n",
    "\n",
    "model2 = Word2Vec(vector_size=200, window=5, min_count=1, sg=0, workers=cores-1)\n",
    "model2.build_vocab(train_data['concatenated_description'], progress_per=1000)\n",
    "t = time.time()\n",
    "model2.train(train_data['concatenated_description'], total_examples=model2.corpus_count, epochs=20)\n",
    "print('Time to build vocab for Model-2: {} mins'.format(round((time.time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jIy1sBOEOyu7"
   },
   "outputs": [],
   "source": [
    "model2.save(\"word2vec_cbow_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One can directly load the model from here instead of training the models again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYAkgFhiOytT",
    "outputId": "d42bb6bc-14be-4a56-c6e5-8ca71172fd27"
   },
   "outputs": [],
   "source": [
    "#Loading a pretrained model\n",
    "\n",
    "model2 = Word2Vec.load(\"word2vec_cbow_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec2(doc):\n",
    "    return np.mean([model2.wv[word] for word in doc if word in model2.wv.index_to_key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2 = train_data.copy()\n",
    "\n",
    "series = train_data_model2.concatenated_description.apply(avg_word2vec2)\n",
    "\n",
    "df = pd.DataFrame(series.apply(pd.Series))\n",
    "\n",
    "train_data_model2 = pd.concat([train_data_model2, df], axis=1)\n",
    "\n",
    "train_data_model2 = train_data_model2.drop(['concatenated_description'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2.to_csv('avgword2vec_cbow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2 = pd.read_csv('avgword2vec_cbow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model2.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'train_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.100559</td>\n",
       "      <td>-0.614330</td>\n",
       "      <td>0.208602</td>\n",
       "      <td>0.571966</td>\n",
       "      <td>-0.131203</td>\n",
       "      <td>0.016526</td>\n",
       "      <td>0.608419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075749</td>\n",
       "      <td>0.193744</td>\n",
       "      <td>0.494159</td>\n",
       "      <td>-0.073508</td>\n",
       "      <td>0.494414</td>\n",
       "      <td>-0.169989</td>\n",
       "      <td>0.131777</td>\n",
       "      <td>0.145828</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>-0.127633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.329170</td>\n",
       "      <td>-0.350506</td>\n",
       "      <td>0.272558</td>\n",
       "      <td>0.105843</td>\n",
       "      <td>1.208020</td>\n",
       "      <td>1.528792</td>\n",
       "      <td>-0.091454</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.339325</td>\n",
       "      <td>0.835479</td>\n",
       "      <td>0.271060</td>\n",
       "      <td>0.437964</td>\n",
       "      <td>1.127263</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>-0.569234</td>\n",
       "      <td>-0.982572</td>\n",
       "      <td>-0.126194</td>\n",
       "      <td>-0.116404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.240003</td>\n",
       "      <td>-0.041858</td>\n",
       "      <td>0.145329</td>\n",
       "      <td>0.452647</td>\n",
       "      <td>0.760089</td>\n",
       "      <td>0.097341</td>\n",
       "      <td>0.452785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418078</td>\n",
       "      <td>0.219362</td>\n",
       "      <td>0.174552</td>\n",
       "      <td>-0.024960</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>-0.374551</td>\n",
       "      <td>-0.620355</td>\n",
       "      <td>0.631266</td>\n",
       "      <td>-0.212377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.101531</td>\n",
       "      <td>-0.345377</td>\n",
       "      <td>-0.117154</td>\n",
       "      <td>-0.361317</td>\n",
       "      <td>-0.764579</td>\n",
       "      <td>0.917608</td>\n",
       "      <td>0.777097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010613</td>\n",
       "      <td>0.054686</td>\n",
       "      <td>0.057820</td>\n",
       "      <td>1.474908</td>\n",
       "      <td>0.728149</td>\n",
       "      <td>1.006546</td>\n",
       "      <td>-0.213868</td>\n",
       "      <td>0.185430</td>\n",
       "      <td>-0.457993</td>\n",
       "      <td>-0.727977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820777</td>\n",
       "      <td>-0.600387</td>\n",
       "      <td>-0.174789</td>\n",
       "      <td>-0.790316</td>\n",
       "      <td>0.611780</td>\n",
       "      <td>0.271854</td>\n",
       "      <td>0.250769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093859</td>\n",
       "      <td>0.078621</td>\n",
       "      <td>-0.048399</td>\n",
       "      <td>-0.540433</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.234550</td>\n",
       "      <td>-0.283534</td>\n",
       "      <td>-0.156189</td>\n",
       "      <td>0.549295</td>\n",
       "      <td>-0.314060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370407</th>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.484753</td>\n",
       "      <td>0.030433</td>\n",
       "      <td>-0.842793</td>\n",
       "      <td>-0.549632</td>\n",
       "      <td>0.861718</td>\n",
       "      <td>1.395888</td>\n",
       "      <td>1.131525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391027</td>\n",
       "      <td>0.211749</td>\n",
       "      <td>-0.072331</td>\n",
       "      <td>1.241811</td>\n",
       "      <td>2.145595</td>\n",
       "      <td>1.513619</td>\n",
       "      <td>1.109203</td>\n",
       "      <td>-1.061759</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>-1.878718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370408</th>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.989117</td>\n",
       "      <td>-0.492069</td>\n",
       "      <td>-0.002195</td>\n",
       "      <td>0.631189</td>\n",
       "      <td>1.291212</td>\n",
       "      <td>-0.071031</td>\n",
       "      <td>0.779076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386589</td>\n",
       "      <td>0.272975</td>\n",
       "      <td>-0.883474</td>\n",
       "      <td>-0.305047</td>\n",
       "      <td>-1.207003</td>\n",
       "      <td>0.288548</td>\n",
       "      <td>-1.372537</td>\n",
       "      <td>-0.139790</td>\n",
       "      <td>1.139007</td>\n",
       "      <td>0.010722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370409</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.183705</td>\n",
       "      <td>0.060849</td>\n",
       "      <td>0.345536</td>\n",
       "      <td>0.088263</td>\n",
       "      <td>0.149190</td>\n",
       "      <td>0.014507</td>\n",
       "      <td>0.373666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325529</td>\n",
       "      <td>0.767687</td>\n",
       "      <td>0.523542</td>\n",
       "      <td>0.516035</td>\n",
       "      <td>0.218952</td>\n",
       "      <td>0.475278</td>\n",
       "      <td>-0.767223</td>\n",
       "      <td>0.097173</td>\n",
       "      <td>-0.474679</td>\n",
       "      <td>-0.750847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370410</th>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.263113</td>\n",
       "      <td>-0.582247</td>\n",
       "      <td>0.410359</td>\n",
       "      <td>-0.201431</td>\n",
       "      <td>0.439368</td>\n",
       "      <td>1.350481</td>\n",
       "      <td>0.880811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339725</td>\n",
       "      <td>0.607348</td>\n",
       "      <td>-0.159939</td>\n",
       "      <td>0.906151</td>\n",
       "      <td>1.756508</td>\n",
       "      <td>0.985768</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>-0.796640</td>\n",
       "      <td>-0.361518</td>\n",
       "      <td>-1.248246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370411</th>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.829921</td>\n",
       "      <td>-0.804902</td>\n",
       "      <td>0.084474</td>\n",
       "      <td>0.184616</td>\n",
       "      <td>0.737504</td>\n",
       "      <td>1.492387</td>\n",
       "      <td>-0.328706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602713</td>\n",
       "      <td>-0.324386</td>\n",
       "      <td>-0.225616</td>\n",
       "      <td>0.208637</td>\n",
       "      <td>-0.016791</td>\n",
       "      <td>-0.252122</td>\n",
       "      <td>-0.543555</td>\n",
       "      <td>0.542459</td>\n",
       "      <td>-0.926188</td>\n",
       "      <td>-1.731979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370412 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_condition_id  price  shipping         0         1         2  \\\n",
       "0                       2    8.0         1 -0.100559 -0.614330  0.208602   \n",
       "1                       3   14.0         0 -0.329170 -0.350506  0.272558   \n",
       "2                       1   25.0         1 -0.240003 -0.041858  0.145329   \n",
       "3                       3   25.0         0 -0.101531 -0.345377 -0.117154   \n",
       "4                       1   12.0         1  0.820777 -0.600387 -0.174789   \n",
       "...                   ...    ...       ...       ...       ...       ...   \n",
       "370407                  1   20.0         1  0.484753  0.030433 -0.842793   \n",
       "370408                  1   44.0         0 -0.989117 -0.492069 -0.002195   \n",
       "370409                  3   15.0         1  0.183705  0.060849  0.345536   \n",
       "370410                  1   16.0         0 -0.263113 -0.582247  0.410359   \n",
       "370411                  1   17.0         1 -1.829921 -0.804902  0.084474   \n",
       "\n",
       "               3         4         5         6  ...       190       191  \\\n",
       "0       0.571966 -0.131203  0.016526  0.608419  ... -0.075749  0.193744   \n",
       "1       0.105843  1.208020  1.528792 -0.091454  ... -1.339325  0.835479   \n",
       "2       0.452647  0.760089  0.097341  0.452785  ...  0.418078  0.219362   \n",
       "3      -0.361317 -0.764579  0.917608  0.777097  ... -0.010613  0.054686   \n",
       "4      -0.790316  0.611780  0.271854  0.250769  ... -0.093859  0.078621   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "370407 -0.549632  0.861718  1.395888  1.131525  ... -0.391027  0.211749   \n",
       "370408  0.631189  1.291212 -0.071031  0.779076  ...  0.386589  0.272975   \n",
       "370409  0.088263  0.149190  0.014507  0.373666  ... -0.325529  0.767687   \n",
       "370410 -0.201431  0.439368  1.350481  0.880811  ... -0.339725  0.607348   \n",
       "370411  0.184616  0.737504  1.492387 -0.328706  ...  0.602713 -0.324386   \n",
       "\n",
       "             192       193       194       195       196       197       198  \\\n",
       "0       0.494159 -0.073508  0.494414 -0.169989  0.131777  0.145828  0.024475   \n",
       "1       0.271060  0.437964  1.127263  1.099126 -0.569234 -0.982572 -0.126194   \n",
       "2       0.174552 -0.024960 -0.002166  0.013162 -0.374551 -0.620355  0.631266   \n",
       "3       0.057820  1.474908  0.728149  1.006546 -0.213868  0.185430 -0.457993   \n",
       "4      -0.048399 -0.540433  0.852686  0.234550 -0.283534 -0.156189  0.549295   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "370407 -0.072331  1.241811  2.145595  1.513619  1.109203 -1.061759  0.059616   \n",
       "370408 -0.883474 -0.305047 -1.207003  0.288548 -1.372537 -0.139790  1.139007   \n",
       "370409  0.523542  0.516035  0.218952  0.475278 -0.767223  0.097173 -0.474679   \n",
       "370410 -0.159939  0.906151  1.756508  0.985768  0.587361 -0.796640 -0.361518   \n",
       "370411 -0.225616  0.208637 -0.016791 -0.252122 -0.543555  0.542459 -0.926188   \n",
       "\n",
       "             199  \n",
       "0      -0.127633  \n",
       "1      -0.116404  \n",
       "2      -0.212377  \n",
       "3      -0.727977  \n",
       "4      -0.314060  \n",
       "...          ...  \n",
       "370407 -1.878718  \n",
       "370408  0.010722  \n",
       "370409 -0.750847  \n",
       "370410 -1.248246  \n",
       "370411 -1.731979  \n",
       "\n",
       "[370412 rows x 203 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"avgword2vec_sg.csv\" --> Will be used for \"train_test_split\", Contains 0.03 of entire dataset. \n",
    "## Generated from extracting dataset \"preprocessed_tokenized_training_data.csv\" and then applying \"word2vec_cbow_model\" on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We've our dataset ready for the Model-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data_model2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e69f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['price']\n",
    "X = data.drop(columns=['price'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "X_test = X_test[:3500]\n",
    "y_test = y_test[:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=100)\n",
    "X_K_train = kpca.fit_transform(X_train)\n",
    "X_K_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.280 total time=  25.0s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.231 total time=  24.5s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.227 total time=  24.2s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.252 total time=  24.4s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.01, max_depth=7, min_child_weight=7, n_estimators=400, reg_alpha=0.1, reg_lambda=0.001, subsample=0.9;, score=0.203 total time=  24.3s\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.193 total time=  16.3s\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.015 total time=  15.9s\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.145 total time=  16.0s\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.094 total time=  16.3s\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=5, n_estimators=200, reg_alpha=0.001, reg_lambda=0.001, subsample=0.6;, score=0.049 total time=  15.9s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.307 total time=  16.1s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.235 total time=  16.1s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.233 total time=  16.7s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.262 total time=  16.4s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0.01, reg_lambda=0.001, subsample=0.8;, score=0.212 total time=  16.2s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.283 total time=  11.7s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.217 total time=  11.1s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.217 total time=  10.9s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.236 total time=  10.5s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.1, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.01, reg_lambda=0, subsample=0.9;, score=0.194 total time=  11.9s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.240 total time=  25.8s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.072 total time=  25.1s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.133 total time=  25.4s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.140 total time=  25.0s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=300, reg_alpha=0, reg_lambda=0.1, subsample=0.9;, score=0.094 total time=  25.0s\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.159 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.129 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.124 total time=   8.5s\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.149 total time=   8.6s\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=0.3, learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, reg_alpha=0.01, reg_lambda=0.001, subsample=0.6;, score=0.122 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.290 total time=  39.4s\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.206 total time=  39.5s\n",
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.195 total time=  39.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.217 total time=  39.5s\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.4, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=400, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.163 total time=  39.3s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.313 total time=  19.7s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.233 total time=  19.7s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.253 total time=  21.5s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.247 total time=  20.0s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=5, n_estimators=300, reg_alpha=0.1, reg_lambda=0.01, subsample=0.8;, score=0.199 total time=  19.8s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.250 total time=  19.9s\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.111 total time=  20.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.190 total time=  20.0s\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.101 total time=  21.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.2, max_depth=5, min_child_weight=7, n_estimators=400, reg_alpha=0, reg_lambda=0.1, subsample=0.6;, score=0.032 total time=  20.0s\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.230 total time=   6.0s\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.204 total time=   6.0s\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.178 total time=   6.1s\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.203 total time=   6.0s\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=0, learning_rate=0.05, max_depth=4, min_child_weight=1, n_estimators=100, reg_alpha=0.01, reg_lambda=1, subsample=1.0;, score=0.163 total time=   6.0s\n",
      "[CV 1/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.275 total time=  13.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.207 total time=  13.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.221 total time=  13.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.233 total time=  13.2s\n",
      "[CV 5/5] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.05, max_depth=4, min_child_weight=3, n_estimators=300, reg_alpha=0, reg_lambda=0.001, subsample=0.7;, score=0.178 total time=  14.1s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.211 total time=  10.2s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.182 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.170 total time=  10.1s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.193 total time=  10.1s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0, subsample=0.9;, score=0.165 total time=  10.1s\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.273 total time=  28.4s\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.229 total time=  28.3s\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.199 total time=  28.4s\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.248 total time=  28.4s\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=7, n_estimators=400, reg_alpha=0.001, reg_lambda=0.01, subsample=0.7;, score=0.186 total time=  29.0s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.211 total time=  15.6s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.112 total time=  15.7s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.110 total time=  15.7s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.128 total time=  16.4s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.4, learning_rate=0.2, max_depth=9, min_child_weight=1, n_estimators=200, reg_alpha=0.001, reg_lambda=0.1, subsample=0.8;, score=0.041 total time=  15.7s\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.252 total time=   6.3s\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.159 total time=   6.1s\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.216 total time=   6.7s\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.195 total time=   6.2s\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0, learning_rate=0.1, max_depth=9, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=0.162 total time=   6.1s\n",
      "Best Hyperparameters: {'subsample': 0.8, 'reg_lambda': 0.001, 'reg_alpha': 0.01, 'n_estimators': 400, 'min_child_weight': 7, 'max_depth': 5, 'learning_rate': 0.05, 'gamma': 0.2, 'colsample_bytree': 0.6}\n",
      "Mean Squared Error: 1712.6221890218992\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate for boosting\n",
    "    'max_depth': range(3, 10),  # Maximum tree depth\n",
    "    'n_estimators': [100, 200, 300, 400],  # Number of boosted trees\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.1, 1],  # L1 regularization term on weights\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.1, 1],  # L2 regularization term on weights\n",
    "    'min_child_weight': [1, 3, 5, 7],  # Minimum sum of instance weight needed in a child\n",
    "}\n",
    "\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist, n_iter=15, cv=5, random_state=42, verbose=3)\n",
    "\n",
    "random_search.fit(X_K_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "y_pred = best_model.predict(X_K_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1712.6221890218992\n",
      "Root Mean Squared Error (RMSE): 41.38383970853719\n",
      "Mean Absolute Error (MAE): 15.28734421351552\n",
      "R-squared (R²): 0.17548515745565463\n",
      "Explained Variance Score: 0.17550200413843597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# R-squared (R²) or Coefficient of Determination\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R²):\", r2)\n",
    "\n",
    "# Explained Variance Score\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "print(\"Explained Variance Score:\", explained_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE: 1091.8808578019537\n",
      "Root Mean Squared Error (RMSE): 33.04362053107912\n",
      "Standard Deviation of MSE: 168.59362554623254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_folds = 5\n",
    "scores = cross_val_score(xgb, X_K_train, y_train, cv=num_folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Cross-validation returns negative mean squared error, so we take the absolute values\n",
    "mse_scores = -scores\n",
    "\n",
    "# Compute the mean and standard deviation of the cross-validation scores\n",
    "mean_mse = mse_scores.mean()\n",
    "std_mse = mse_scores.std()\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mean_mse)\n",
    "# Print the results\n",
    "print(\"Mean MSE:\", mean_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Standard Deviation of MSE:\", std_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model6.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(xgb, file)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
